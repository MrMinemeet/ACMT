\chapter{Workflow\authorA/\authorB}


% USED HARDWARE
\section{Used Hardware\authorA}
For video capturing a \textit{Raspberry Pi 3B+} with a \textit{Raspberry Pi Camera V2} or a \textit{Raspberry Pi Wide Angle Lense Camera} are used. The Raspberry Pi sends the video feed to a separate more power full PC over WiFi using a Python3 script. \newline
For processing a \textit{Lenovo ThinkStation S20} or a \textit{Lenovo W550s} are used depending on the amount of processing power is required. For more intense work a server access at the Johannes Kepler University was supplied to work on their system. \newline
As the work is based around implementing it on the Audi Autonomous Driving Cup (\gls{aadc}) car a remote controlled model car was borrowed for a few weeks.

% USED SOFTWARE
\section{Used Software\authorA}
\subsection{Raspberry Pi}
The Raspberry Pi is running Raspbian Buster since it is well optimized for the mini computer and only required to be able to execute a Python script to send the raw video feed over http to the the processing device.\newline
\subsection{PC}
The ThinkStation and the laptop are running Kubuntu 18.04, which is basically Ubuntu but has a GUI that's a more like Windows and is supported until May 2023. \newline
The ThinkStation has a eight core Intel Xeon CPU, a GTX 1660TI and 12GB of RAM inside. \newline
The Laptop has a four core Intel i7 and 8GB of RAM built in.\newline

\textbf{\underline{ADTF}} \newline
At first Ubuntu 16.04 with Automotive Data and Time-Triggered Framework (\gls{adtf}) was used since it's the recommended environment by the \gls{aadc} car manufacturer DigitalWerk. There were many compatibility ans stability issues and it is very difficult to get into the whole system as it's not very beginner friendly. Students at the JKU said that they spent at least four weeks to know how it somewhat worked. After trying to get the basics of \gls{adtf} working it was clear that switching to ROS might be better.

\textbf{\underline{ROS}} \newline
Running ROS Melodic on Kubuntu 18.04 was pretty straight forward. The instructions on the ROS website are very clear and can be directly copied without issues. The principle of the workspace is also easy to understand. In the source folder the modules get put in and when compiling the modules automatically generates a setup file to use them.

% SENDING IMAGE FROM RASPBERRY PI TO PC/LAPTOP
\section{Setup\authorA}
As the PC and laptop are not the best idea to run around with, a Raspberry Pi is used instead to stream the video over WiFi to the PC/laptop which are connected to the router over LAN. This makes the camera setup very portable as the pi, camera and powerbank are packed together and don't have much weight. The PC can sit somewhere where and just processing the received video signal.

\subsection{Streaming video from Pi to PC}

\textbf{\underline{Enabling Camera}}\newline
To use a camera on a Raspberry Pi the interface needs to be enabled first. This can be done in the built-in tool called \textit{raspi-config}. In this tool under the subsection called \textit{Interfacing Options} there is a option with the name \textit{Camera}. When this is done the camera can be used after a restart.

\textbf{\underline{Python Script}}\newline
In the code snipped \ref{code:videostreamMain} at first a \textit{piCamera} instance with the name \textit{cam} is created. As parameters the resolution gets set to \textit{1280x720} pixels and the framerate is set to \textit{30} frames per second (\gls{fps}). If needed the image can be rotated, e.g the camera is mounted upside down. When starting the camera a output and format are expected. For the output a separate class is used which sets how and when a new frame can be published and for the format the \textit{mjpeg} video codec is chosen, as a pack for getting mjpeg-streams already exists in \gls{ros} and it's not power hungry when running it on the Raspberry Pi. \newline
After the camera \enquote{recording} has started successfully the server is started to make the stream accessible to other devices. The server runs until the user closes the script using \textit{CTRL + C}. After closing the server the \textit{finally} block gets called, where the camera \enquote{recording} is stopped so that other programs can use the camera again. \newline

\lstinputlisting[language=Python, firstline=66, lastline=80,caption={Main Function of CamStream Feed},label={code:videostreamMain}]{./media/code_snippets/HTTPCamStream.py}

The streamingHandler that is shown in snipped \ref{code:videoStreamHandler} handles the actions that are taken when client connects to the RaspberryPi. At the beginning it checks if the client is requesting the \textit{/stream.mjpg} file. If the client is not requesting that specific file a \textit{404 Not Found} Error is returned. But if the correct file is requested at first a \textit{200 OK} code. In addition to the status code headers are send, which tell the client to not use cache. After sending the HTTP OK to the client a permanent loop is startet which always waits until a new image from the camera is ready and then sends it to the client as an jpeg image. The loop ensures that the always client receives the latest image and so creates a video. Should the client drop the connection an exception is raised which causes the loop to stop and end the handler for that specific client until the client connects again. \newline
\lstinputlisting[language=Python, firstline=31, lastline=62, caption={StreamingHandler of CamStream},label={code:videoStreamHandler}]{./media/code_snippets/HTTPCamStream.py}