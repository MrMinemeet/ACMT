% !TEX root = ../Vorlage_DA.tex
%	########################################################
% 							Vorwort
%	########################################################


%	--------------------------------------------------------
% 	Überschrift, Inhaltsverzeichnis
%	--------------------------------------------------------
\chapter*{Abstract}
% ENGLISH
A big problem for self-driving cars is to analyze the environment it is in and how to process the incoming data to get the most information out of it.  Why is this such an important task when we already have technology like Global Positioning System(\gls{gps})? \gls{gps} provides routing data, but no live details what happens on the street, like an ongoing construction or a pedestrian crossing the street. Hence there is no dynamic information in the \gls{gps} data. To get this information the car has to analyze its environment in real-time. This paper focuses on the problem when the input data is fetched by a camera and thus only contains visual data. There were three different ways tested. They all have in common that they use the data to generate a digital map and show the position off the car itself in this map. A \gls{slam} is a algorithm that analyzes the environment through a camera to get depth information and generate a map in which the car can localize itself. The first two methods ORB-\gls{slam} and LSD-\gls{slam} are mathematical algorithms. The platform for ORB-\gls{slam} and LSD-\gls{slam} is the Robot Operating System (\gls{ros}), which handles the communication between the camera and the \gls{slam} algorithms. The third method DeepTAM uses Deep Neural Networks which can learn to predict the distance between objects in a image when given enough training data. It is a system for keyframe-based dense camera tracking and depth map estimation that is entirely learned. This thesis shows how to implement the different algorithms in \gls{ros} and points out the pros and cons of each method compared to the others and gives an outlook into future methods.
\newline
\newline
\newline
\newline
% GERMAN
Ein großes Problem für selbstfahrende Autos ist es, die Umgebung, in der sich das Auto befindet, zu analysieren und die eingehenden Daten zu verarbeiten, um das Maximum an Informationen herauszuholen.  Warum ist dies eine so wichtige Aufgabe, wenn wir bereits über Technologie wie \gls{gps} verfügen? Weil \gls{gps} nicht immer so genau oder gar nicht verfügbar ist und die Informationen immer auf dem neuesten Stand sein müssen (z.B. Änderungen aufgrund einer Baustelle). Diese Arbeit konzentriert sich auf das Problme, wenn die Eingangsdaten von einer Kamera geholt weden und somit nur visuelle Daten enthalten. Es wurden drei verschiedene Methoden getestet. Alle haben gemeinsam, das sie die Daten zur Erstellung einer digitalen Karte verwenden und die Position des Fahrzeugs selbst in dieser Karte anzeigen. Die ersten beiden Methoden ORB-SLAM und LSD-SLAM sind mathematische Algorithmen. Beide arbeiten mit dem für den praktischen Einsatz wichtigen Robot Operating System (ROS). Die dritte Methode DeepTAM verwendet ein sogenanntes Deep Neural Network, welches aus genug Trainingsdaten lernen kann. Es handelt es sich um ein Keyframe basiertes System von Kameraverfolgung und Tiefenschätzung zur Generierung von 3D Karten. Diese Arbeit vergleicht und zeigt die Vor- und Nachteile der einzelnen Methoden. Außerdem gibt sie einen Blick in zukünftige Angehensweisen.
 

\addcontentsline{toc}{chapter}{Abstract}